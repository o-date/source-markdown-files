## Linked Open Data and Data Publishing

Digital archaeology is wonderfully messy, and it pulls in many different ways. This messiness can prompt a variety of responses. @hugget_locke_2018 try to put a framework around this messiness so that we, as a field, can understand where we're going and work out where we might like to be. One element they draw attention to is the use of tools from other, non-archaeological, sources and the unintended side-effects that such tool use can have:

> _what must we do in order to know when we know something new?_ Without this fundamental capability there is little possibility of locating the so-called ‘something new’ and determining whether it is really valuable, or merely useful, or have the opportunity to critique, shape or incorporate that ‘new knowledge’ to the benefit of the broader community. Secondly, _how do we identify gaps and obstacles to accessing our total potential pool of knowledge and capabilities?_ [...]

> Clearly then, this bricolage of different institutional, private and individual knowledge bases makes it very difficult to see the gaps and to know ‘when we know something new’, whether it be new data, a discovery, a novel approach, an insight or finding and, especially, where, or with whom, this new or potential knowledge resides. This suggests the need for some level of knowledge brokerage [...]

The [Linked Ancient World Data Institute](http://wiki.digitalclassicist.org/Linked_Ancient_World_Data_Institute) is one answer to this problem of data brokerage. Over a series of meetings (and through the work of the [Pelagios Commons](http://commons.pelagios.org/)) the participants developed approaches to using the principles of Linked Open Data to '[fill in the gaps](https://github.com/lawdi/LAWD)' in establed ontologies (or descriptions of data) for use with Greco-Roman antiquity. 

What does that mean, and how does that work? While many databases, services, or museums might expose their data via a web API, there can be limitations. Matthew Lincoln has an excellent tutorial at [The Programming Historian](https://programminghistorian.org/en/lessons/graph-databases-and-SPARQL) that walks us through some of these differences, but the key one is in the way the data is represented. When data is described using a 'Resource Description Framework', RDF, the resource - the 'thing'- is described via a series of relationships, rather than as rows in a table or keys having values. In this approach, 'things' and 'concepts' are linked to unique descriptions of identifiers. In this way, data that is housed in one location can be integrated with data in another on the basis of semantic meanings.

Information is in the relationships. It's a network. It's a _graph_. Thus, every 'thing' in this graph can have its own _uniform resource identifier_ (URI) that lives as a location on the internet. Information can then be created by making _statements_ that use these URIs, similarly to how English grammar creates meaning: subject verb object. Or, in RDF-speak, 'subject predicate object', also known as a _triple_. In this way, data in _different_ places can be linked together by referencing the elements they have in common. This is Linked Open Data (LOD). The access point for interrogating LOD is called an 'endpoint'. 

To ask questions of the data, we write queries of the host computer which we access at the endpoit. A common language for writing such queries is _SPARQL_ which stands for for SPARQL Protocol and RDF Query Language (yes, it's one of _those_ kinds of acronyms).

In the notebook for this section, we're not using Python or R directly. Instead, we've set up a 'kernel' (think of that as the 'engine' for the notebook) that already includes everything necessary to set up and run SPARQL queries. (For reference, the kernel code is [here](https://github.com/paulovn/sparql-kernel)). Both R and Python can interact with and query endpoints, and manipulate linked open data, but for the sake of learning a bit of what one can do with SPARQL, this notebook keeps all of that ancillary code tucked away. The followup notebook shows you how to use R to do some basic manipulations of the query results.

### exercises

1. Walk through the notebook to get the hang of writing simple queries of the various endpoints that it is configured to use. Then, modify the existing queries to retrieve answers to your own questions. What are some of the hidden 'gotchas' that you encounter, and what else do you need to know in order to make efficient use of the resources that _do_ exist for linked open archaeological data?
2. Ethan Gruber is a leading authority on using linked open data for archaeological work. [In this post](https://numishare.blogspot.com/2018/02/a-closer-look-at-research-space.html) he explores [ResearchSpace](http://researchspace.org/) from the British Museum. Consider his criticisms and explore ResearchSpace for yourself. 